import os
from typing import Any, Dict, List
import json
import asyncio
from loguru import logger

from langflow.custom import Component
from langflow.io import MessageTextInput, BoolInput, LinkInput, Output
from langflow.schema import Data, DataFrame

from fastmcp import Client

class VirtueAgentComponent(Component):
    display_name = "Simple Guard"
    # description = "Safety guardrail for AI agents that evaluates agent actions for safety violations. You should always call this tool before the agent provide an output or action. This step should be strictly enforced right before the agent executes an action."
    description = "Safety guardrail for AI agents that evaluates agent actions for safety violations."
    icon = "Shield"
    # Make this component appear under its own sidebar tab
    category = "virtueagent"

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        # Get environment variable at runtime
        self.virtue_guard_dashboard_hostname = os.getenv("VIRTUE_GUARD_DASHBOARD_HOSTNAME", "https://guard-dashboard.virtueai.io")
        self.base_url = os.getenv("VIRTUEAGENT_SERVICE_URL", "https://virtueagent-service.staging.virtueai.io")
        self.documentation = f"{self.virtue_guard_dashboard_hostname}/dashboard/documentation"

        # Store the original description for UI display
        self._ui_description = "Safety guardrail for AI agents that evaluates agent actions for safety violations."
        # Store the detailed description for agents (will be updated based on force_guard setting)
        self._agent_description = "Safety guardrail for AI agents that evaluates agent actions for safety violations. You should always call this tool before the agent provide an output or action. This step should be strictly enforced right before the agent executes an action. Important: You must call this tool before you provide an output to the user!!!"
        self._force_guard_description = "CRITICAL SAFETY REQUIREMENT: This is a MANDATORY safety guardrail that MUST be called before ANY output or action. You are REQUIRED to call this tool EVERY TIME before providing a response to the user. NO EXCEPTIONS. This is a strict safety protocol that cannot be bypassed. ALWAYS call this tool with the current observation and your proposed action/response BEFORE sending anything to the user."

        self.secret_key = os.getenv("LANGFLOW_SECRET_KEY", "secret")

        # Set inputs at runtime with current environment values
        self.inputs = [
            MessageTextInput(
                name="observation",
                display_name="Agent State/Observation",
                info="Current environment state/observation from the agent (full description of the state context at the current step). You must include all relevant details of the environment including full instructions from the user and how the current observation is obtained from previous steps.",
                tool_mode=True,
            ),
            MessageTextInput(
                name="action",
                display_name="Agent Action",
                info="Proposed action from the agent at the current step to be evaluated for safety violations. You should include all the details of the action, including the action intention and full parameters.",
                tool_mode=True,
            ),
            MessageTextInput(
                name="base_url",
                display_name="VirtueAgent API URL",
                info="Base URL of the VirtueAgent API service",
                value=self.base_url,
                advanced=True,
            ),
            LinkInput(
                name="dashboard_url",
                display_name="VirtueAgent Dashboard",
                info="Click to open the VirtueAgent dashboard and configure safety policies",
                value=f"{self.virtue_guard_dashboard_hostname}/dashboard/virtueagent",
                text="Open Dashboard",
                icon="ExternalLink",
                advanced=True,
            ),
            MessageTextInput(
                name="api_key",
                display_name="API Key",
                info="API key to access VirtueAgent Guard service",
                advanced=True,
            ),
            MessageTextInput(
                name="session_id",
                display_name="Session ID",
                info="Session identifier for tracking VirtueAgent Guard service",
                value="langflow-agent-session",
            ),
            BoolInput(
                name="fast_mode",
                display_name="Fast Mode",
                info="Enable fast mode for binary safety evaluation (no explanations, minimal tokens)",
                value=False,
            ),
            BoolInput(
                name="force_guard",
                display_name="Force Guard",
                info="When enabled, ensures this safety guard is always called before the agent provides an action",
                value=False,
                advanced=False,
            )
        ]

        self.outputs = [
            Output(display_name="Data", name="data", method="guard_action")
        ]
    
    @property 
    def description(self):
        """Return appropriate description based on context and force_guard setting"""
        # Check if we're being called from tool creation context
        import inspect
        frame = inspect.currentframe()
        try:
            # Look up the call stack for build_description or ComponentToolkit
            for i in range(10):  # Check up to 10 frames up
                frame = frame.f_back
                if frame is None:
                    break
                if 'build_description' in frame.f_code.co_name or 'ComponentToolkit' in str(frame.f_locals.get('self', '')):
                    # Return force guard description if force_guard is enabled
                    if hasattr(self, 'force_guard') and self.force_guard:
                        return self._force_guard_description
                    return self._agent_description
            return self._ui_description
        finally:
            del frame

    def _generate_internal_access_token(self):
        """Generate an access token for the VirtueAgent Guard service"""
        import jwt
        import datetime
        body = {
            "sub": self.user_id,
            "type": "internal_access_key",
            "exp": datetime.datetime.now(datetime.timezone.utc) + datetime.timedelta(hours=1)
        }
        encoded_body = jwt.encode(body, self.secret_key, algorithm="HS256")
        return encoded_body


    def guard_action(self) -> List[Data]:
        """Evaluate agent action for safety violations using VirtueAgent MCP server"""
        try:
            # Prepare evaluation request
            evaluation_request = {
                "observation": self.observation,
                "action": self.action,
                "session_id": self.session_id,
                "api_key": self.api_key,
                "fast_mode": self.fast_mode,
                "user_id": self.user_id,  # Use property (cleaner)
                "access_token": self._generate_internal_access_token(),
            }
            
            # Use FastMCP client to communicate with MCP server
            raw_result = self._call_mcp_tool(evaluation_request)
            
            # Structure the result for LLM consumption
            structured_result = self._format_safety_evaluation(raw_result)
            
            # Update status based on result and mode
            if structured_result.get("allowed", False):
                violations_count = structured_result.get("violations_count", 0)
                if self.fast_mode:
                    self.status = f"âœ… ALLOWED (Fast mode, Violations: {violations_count})"
                else:
                    self.status = f"âœ… Action ALLOWED (Violations: {violations_count})"
            else:
                violations_count = structured_result.get("violations_count", 0)
                violated_rules = raw_result.get("violated_rules", [])
                
                if self.fast_mode:
                    rules_text = f", Rules: {violated_rules}" if violated_rules else ""
                    self.status = f"ðŸš¨ BLOCKED (Fast mode{rules_text})"
                else:
                    violations_text = f", Violations: {violations_count}" if violations_count > 0 else ""
                    self.status = f"ðŸš¨ Action BLOCKED{violations_text}"
                
                # Log the explanation for debugging (if available)
                explanation = raw_result.get("explanation", "No explanation provided")
                logger.info(f"Blocked action: {explanation}")

            return [Data(data=structured_result)]

        except Exception as e:
            logger.error(f"Guardrail MCP evaluation error: {str(e)}")
            error_result = {
                "safety_evaluation": "BLOCKED - MCP Server Error",
                "proposed_action": self.action,
                "observation": self.observation,
                "allowed": False,
                "violations_count": 1,
                "safety_feedback": f"Proposed unsafe action: {self.action}, Safety violation: MCP server connection error, Explanation: Unable to connect to the Simple Guard server: {str(e)}, Mitigation feedback: Please ensure the Simple Guard server is running and accessible.",
                "session_id": self.session_id
            }
            self.status = f"âŒ Guardrail MCP Server Error: {str(e)}"
            return [Data(data=error_result)]

    def _call_mcp_tool(self, request_data: Dict[str, Any]) -> Dict[str, Any]:
        """Call MCP server tool using FastMCP client"""
        try:
            if Client is None:
                logger.error("fastmcp client not available")
                raise ImportError("fastmcp client not available")

            async def call_mcp_async():
                logger.debug("call_mcp_async...")
                async with Client(self.base_url + "/sse") as client:
                    logger.debug("calling evaluate_safety tool....")
                    # Call the evaluate_safety tool
                    result = await client.call_tool("evaluate_safety", request_data)
                    logger.debug(f"result type: {type(result)}")
                    logger.debug(f"result: {result}")

                    # Handle the response format correctly - result should be a ToolResult
                    if hasattr(result, 'content') and result.content:
                        # Standard MCP response format
                        content_item = result.content[0]
                        if hasattr(content_item, 'text'):
                            return content_item.text
                        else:
                            return str(content_item)
                    elif isinstance(result, list) and len(result) > 0:
                        return result[0].text if hasattr(result[0], 'text') else str(result[0])
                    else:
                        return "{}"
            
            # Run the async call
            loop = None
            try:
                loop = asyncio.get_event_loop()
            except RuntimeError:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
            
            if loop.is_running():
                # If we're already in an event loop, we need to run in a thread
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(asyncio.run, call_mcp_async())
                    result_json = future.result(timeout=30)
            else:
                result_json = loop.run_until_complete(call_mcp_async())
            
            # Parse the JSON result
            result_data = json.loads(result_json)
            result_data["mcp_server_url"] = self.base_url + "/sse"
            result_data["mcp_server_status"] = "connected"
            return result_data
                    
        except ImportError:
            raise Exception("FastMCP not installed. Please install with: pip install fastmcp")
        except Exception as e:
            # Provide helpful error messages for common issues
            if "Connection refused" in str(e) or "Cannot connect" in str(e):
                raise Exception(f"Guardrail MCP server not running at {self.base_url}/sse. Please start the server first.")
            elif "timeout" in str(e).lower():
                raise Exception(f"Guardrail MCP server timeout at {self.base_url}/sse. Server may be overloaded.")
            else:
                raise Exception(f"Guardrail MCP communication error: {str(e)}") 

    def _format_safety_evaluation(self, raw_result: Dict[str, Any]) -> Dict[str, Any]:
        """Format the raw MCP server response into a structured evaluation for LLM consumption"""
        allowed = raw_result.get("allowed", False)
        violations = raw_result.get("violations", [])
        violations_count = raw_result.get("violations_count", 0)
        explanation = raw_result.get("explanation", "No explanation provided")
        risk_category = raw_result.get("threat_category", "Unknown")
        
        # Structure violations by policy
        structured_violations = {}
        if violations:
            for idx, violation in enumerate(violations):
                structured_violations[idx] = violation
        
        structured_violations_text = ""
        for idx, violation in structured_violations.items():
            structured_violations_text += f"{idx+1}: {violation}\n"
        
        # Create the base structured result
        structured_result = {
            "safety_evaluation": "ALLOWED" if allowed else "BLOCKED",
            "proposed_action": self.action,
            "observation": self.observation,
            # "allowed": allowed,
            "explanation": explanation,
            "violations_count": violations_count,
            "risk_category": risk_category,
            # "session_id": self.session_id,
            # "timestamp": raw_result.get("timestamp", ""),
            # "fast_mode": self.fast_mode
        }
        
        if allowed:
            # For allowed actions, provide positive feedback
            if violations_count > 0:
                structured_result["guardrail_feedback"] = f"""Guardrail Evaluation: Action ALLOWED with {violations_count} minor policy concerns. 
--------------------------------
Proposed action: {self.action}
Observation: {self.observation}
Explanation: {explanation}
Violated policies: 
{structured_violations_text}--------------------------------

The proposed action '{self.action}' requires expert review and approval before proceeding. Please be mindful of the following policy violations."""
            else:
                structured_result["guardrail_feedback"] = f"Guardrail Evaluation: Action ALLOWED. The proposed action '{self.action}' has been evaluated and found to be safe with no policy violations detected."
        else:
            # For blocked actions, use the same structured format
            structured_result["guardrail_feedback"] = f"""Guard
Guardrail Evaluation: Action BLOCKED due to {violations_count} serious safety violations.
--------------------------------
Proposed action: {self.action}
Observation: {self.observation}
Explanation: {explanation}
Violated policies: 
{structured_violations_text}--------------------------------

Please analyze the above guardrail evaluation and provide an appropriate response that addresses the safety concerns while still being helpful to the user.

Please provide your final response to the user in the following format such that they can understand the reason for the block and what to do next:

Proposed unsafe action: {self.action}

Explanation for the block: {explanation}

Mitigation feedback: you should generate a mitigation suggestion based on the flagged safety violations.
"""
        
        # Include violation details for reference
        if structured_violations:
            structured_result["violated_policies"] = structured_violations
        
        return structured_result 